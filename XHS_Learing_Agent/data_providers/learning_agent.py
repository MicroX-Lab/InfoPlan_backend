from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
import re
from typing import List, Dict, Optional
from loguru import logger
from config import Config
from model_service.interfaces import DataProvider, NoteInfo


class XHSLearningAgent:
    """å°çº¢ä¹¦å­¦ä¹ è§„åˆ’Agentï¼ˆJupyterå‹å¥½ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, model_name: str = None, data_provider: Optional[DataProvider] = None):
        """
        åˆå§‹åŒ–å­¦ä¹ Agent
        :param model_name: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        :param data_provider: æ•°æ®æä¾›è€…ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨MockDataProvider
        """
        self.model_name = model_name or Config.MODEL_PATH
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map="auto"
            )
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # æ•°æ®æä¾›è€…ï¼ˆå¯æ’æ‹”ï¼‰
        if data_provider is None:
            from data_providers import MockDataProvider
            self.data_provider = MockDataProvider()
            logger.info("ğŸ“ ä½¿ç”¨Mockæ•°æ®æä¾›è€…")
        else:
            self.data_provider = data_provider
            logger.info("ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æä¾›è€…")
    
    def get_user_notes(self, user_ids: List[str], max_notes_per_user: int = None) -> List[NoteInfo]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„ç¬”è®°åˆ—è¡¨"""
        max_notes = max_notes_per_user or Config.MAX_NOTES_PER_USER
        return self.data_provider.get_user_notes(user_ids, max_notes)
    
    def decompose_goal(self, goal: str, user_notes: List[NoteInfo]) -> Dict:
        """å°†ç”¨æˆ·ç›®æ ‡æ‹†è§£æˆå…·ä½“æ­¥éª¤ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        # åˆ†æç”¨æˆ·ç¬”è®°çš„ä¸»é¢˜åˆ†å¸ƒ
        notes_summary = ""
        note_keywords = []
        
        if user_notes:
            notes_summary = "\nç”¨æˆ·å…³æ³¨çš„ç›¸å…³ç¬”è®°ä¸»é¢˜ï¼š\n"
            for note in user_notes[:10]:
                notes_summary += f"- {note.title}\n"
                # æå–å…³é”®è¯
                note_keywords.extend(note.tags)
                note_keywords.extend(note.title.split())
            
            # å»é‡
            note_keywords = list(set(note_keywords))
            notes_summary += f"\nç›¸å…³å…³é”®è¯ï¼š{', '.join(note_keywords[:10])}"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦ä¹ è§„åˆ’åŠ©æ‰‹ã€‚ç”¨æˆ·æƒ³è¦å®ç°ä»¥ä¸‹ç›®æ ‡ï¼š

ç›®æ ‡ï¼š{goal}
{notes_summary}

è¯·å°†è¿™ä¸ªç›®æ ‡æ‹†è§£æˆ3-5ä¸ªå…·ä½“ã€å¯æ‰§è¡Œçš„æ­¥éª¤ã€‚è¦æ±‚ï¼š
1. æ¯ä¸ªæ­¥éª¤åº”è¯¥å…·ä½“æ˜ç¡®ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ
2. æ­¥éª¤ä¹‹é—´è¦æœ‰é€»è¾‘é¡ºåºï¼ˆä»åŸºç¡€åˆ°è¿›é˜¶ï¼‰
3. æ­¥éª¤å†…å®¹åº”è¯¥ä¸ç›®æ ‡é«˜åº¦ç›¸å…³ï¼ˆ"{goal}"ï¼‰
4. å¦‚æœæä¾›äº†ç¬”è®°ä¸»é¢˜ï¼Œæ­¥éª¤åº”è¯¥å°½é‡åˆ©ç”¨è¿™äº›ç¬”è®°èµ„æº
5. æ­¥éª¤æè¿°è¦ç®€æ´æ˜äº†ï¼Œæ¯ä¸ªæ­¥éª¤ä¸è¶…è¿‡30ä¸ªå­—

è¾“å‡ºæ ¼å¼ä¸ºJSONï¼š
{{
    "steps": [
        "æ­¥éª¤1çš„å…·ä½“æè¿°ï¼ˆä¸ç›®æ ‡ç›´æ¥ç›¸å…³ï¼‰",
        "æ­¥éª¤2çš„å…·ä½“æè¿°ï¼ˆä¸ç›®æ ‡ç›´æ¥ç›¸å…³ï¼‰",
        ...
    ]
}}

é‡è¦æç¤ºï¼š
- æ­¥éª¤å¿…é¡»å›´ç»•ç›®æ ‡"{goal}"å±•å¼€
- ä¸è¦åç¦»ä¸»é¢˜
- åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE
        )
        
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(output_ids, skip_special_tokens=True)
        
        # è§£ææ­¥éª¤
        steps = self._parse_steps_from_response(response, goal)
        
        # åŒ¹é…ç¬”è®°åˆ°æ­¥éª¤ï¼ˆä½¿ç”¨æ”¹è¿›çš„åŒ¹é…é€»è¾‘ï¼‰
        matched_notes = self.match_notes_to_steps(steps, user_notes)
        
        return {
            "goal": goal,
            "steps": steps,
            "matched_notes": matched_notes
        }
    
    def _parse_steps_from_response(self, response: str, goal: str) -> List[str]:
        """ä»æ¨¡å‹å“åº”ä¸­è§£ææ­¥éª¤"""
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                steps = result.get('steps', [])
                if steps:
                    return steps
        except Exception as e:
            logger.warning(f"è§£ææ­¥éª¤å¤±è´¥: {e}")
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
        steps = re.findall(r'\d+[\.ã€]\s*(.+?)(?=\n|$)', response)
        if steps:
            return steps
        
        # æœ€åçš„é™çº§æ–¹æ¡ˆ
        logger.warning("ä½¿ç”¨é»˜è®¤æ­¥éª¤ç”Ÿæˆæ–¹æ¡ˆ")
        return [
            f"äº†è§£{goal}çš„åŸºç¡€çŸ¥è¯†",
            f"å­¦ä¹ {goal}çš„æ ¸å¿ƒæ¦‚å¿µ",
            f"å®è·µ{goal}çš„åŸºæœ¬æ“ä½œ",
            f"æ·±å…¥æŒæ¡{goal}çš„é«˜çº§åº”ç”¨"
        ]
    
    def match_notes_to_steps(self, steps: List[str], notes: List[NoteInfo]) -> Dict[str, List[NoteInfo]]:
        """å°†ç¬”è®°åŒ¹é…åˆ°å„ä¸ªæ­¥éª¤ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        if not notes:
            return {step: [] for step in steps}
        
        notes = notes[:Config.MAX_NOTES_FOR_MATCHING]
        
        # å…ˆå°è¯•ä½¿ç”¨æ¨¡å‹åŒ¹é…
        model_matched = self._match_by_model(steps, notes)
        
        # å¦‚æœæ¨¡å‹åŒ¹é…ç»“æœä¸ºç©ºï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºåå¤‡
        if not any(model_matched.values()):
            logger.info("æ¨¡å‹åŒ¹é…ç»“æœä¸ºç©ºï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…ä½œä¸ºåå¤‡")
            return self._match_by_keywords(steps, notes)
        
        return model_matched
    
    def _match_by_keywords(self, steps: List[str], notes: List[NoteInfo]) -> Dict[str, List[NoteInfo]]:
        """ä½¿ç”¨å…³é”®è¯åŒ¹é…ç¬”è®°åˆ°æ­¥éª¤"""
        result = {step: [] for step in steps}
        
        # æå–æ­¥éª¤ä¸­çš„å…³é”®è¯
        def extract_keywords(text: str) -> List[str]:
            """æå–å…³é”®è¯"""
            keywords = []
            # æå–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-4ä¸ªå­—ï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
            keywords.extend(chinese_words)
            
            # æå–è‹±æ–‡å…³é”®è¯
            english_words = re.findall(r'\b[A-Za-z]{3,}\b', text)
            keywords.extend([w.lower() for w in english_words])
            
            return keywords
        
        for step in steps:
            step_keywords = extract_keywords(step)
            step_lower = step.lower()
            
            # è®¡ç®—æ¯ä¸ªç¬”è®°çš„ç›¸å…³åº¦åˆ†æ•°
            note_scores = []
            for note in notes:
                score = 0
                note_text = f"{note.title} {note.desc} {' '.join(note.tags)}".lower()
                
                # å…³é”®è¯åŒ¹é…
                for keyword in step_keywords:
                    if keyword.lower() in note_text:
                        score += 2
                
                # æ ‡é¢˜åŒ¹é…ï¼ˆæƒé‡æ›´é«˜ï¼‰
                for keyword in step_keywords:
                    if keyword.lower() in note.title.lower():
                        score += 3
                
                # æ ‡ç­¾åŒ¹é…
                for keyword in step_keywords:
                    if any(keyword.lower() in tag.lower() for tag in note.tags):
                        score += 2
                
                # æ•´ä½“æ–‡æœ¬ç›¸ä¼¼åº¦
                if any(word in step_lower for word in note.title.lower().split()):
                    score += 1
                
                note_scores.append((score, note))
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©å‰Nä¸ª
            note_scores.sort(reverse=True, key=lambda x: x[0])
            matched = [note for score, note in note_scores[:Config.NOTES_PER_STEP] if score > 0]
            result[step] = matched
        
        return result
    
    def _match_by_model(self, steps: List[str], notes: List[NoteInfo]) -> Dict[str, List[NoteInfo]]:
        """ä½¿ç”¨æ¨¡å‹åŒ¹é…ç¬”è®°åˆ°æ­¥éª¤"""
        steps_text = "\n".join([f"{i+1}. {step}" for i, step in enumerate(steps)])
        notes_text = "\n".join([
            f"{i}. {note.title}: {note.desc[:100]}"
            for i, note in enumerate(notes)
        ])
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œéœ€è¦å°†ç¬”è®°åŒ¹é…åˆ°å¯¹åº”çš„å­¦ä¹ æ­¥éª¤ã€‚

å­¦ä¹ æ­¥éª¤ï¼š
{steps_text}

å¯ç”¨ç¬”è®°ï¼ˆæ ¼å¼ï¼šç´¢å¼•. æ ‡é¢˜: æè¿°ï¼‰ï¼š
{notes_text}

è¯·ä¸ºæ¯ä¸ªæ­¥éª¤æ¨èæœ€ç›¸å…³çš„{Config.NOTES_PER_STEP}æ¡ç¬”è®°ã€‚è¾“å‡ºæ ¼å¼ä¸ºJSONï¼š
{{
    "æ­¥éª¤1": [ç¬”è®°ç´¢å¼•åˆ—è¡¨],
    "æ­¥éª¤2": [ç¬”è®°ç´¢å¼•åˆ—è¡¨],
    ...
}}

æ³¨æ„ï¼š
1. ç´¢å¼•ä»0å¼€å§‹
2. æ¯ä¸ªæ­¥éª¤æ¨è{Config.NOTES_PER_STEP}æ¡ç¬”è®°
3. å¦‚æœæŸä¸ªæ­¥éª¤æ²¡æœ‰ç›¸å…³ç¬”è®°ï¼Œå¯ä»¥è¿”å›ç©ºåˆ—è¡¨[]
4. åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
        
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=512,
            temperature=0.3
        )
        
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
        response = self.tokenizer.decode(output_ids, skip_special_tokens=True)
        
        return self._parse_matching_result(response, steps, notes)
    
    def _parse_matching_result(self, response: str, steps: List[str], notes: List[NoteInfo]) -> Dict[str, List[NoteInfo]]:
        """è§£æç¬”è®°åŒ¹é…ç»“æœ"""
        result = {step: [] for step in steps}
        
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                match_result = json.loads(json_match.group())
                for step_idx, step in enumerate(steps, 1):
                    step_key = f"æ­¥éª¤{step_idx}"
                    if step_key in match_result:
                        note_indices = match_result[step_key]
                        if isinstance(note_indices, list):
                            result[step] = [
                                notes[i] for i in note_indices 
                                if isinstance(i, int) and 0 <= i < len(notes)
                            ]
        except Exception as e:
            logger.warning(f"è§£æåŒ¹é…ç»“æœå¤±è´¥: {e}")
        
        return result
    
    def format_output(self, result: Dict) -> str:
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
        output = f"ğŸ“š å­¦ä¹ ç›®æ ‡ï¼š{result['goal']}\n\n"
        output += "=" * 50 + "\n\n"
        
        for idx, step in enumerate(result['steps'], 1):
            output += f"ğŸ“Œ æ­¥éª¤ {idx}ï¼š{step}\n\n"
            
            matched_notes = result['matched_notes'].get(step, [])
            if matched_notes:
                output += "   ç›¸å…³ç¬”è®°æ¨èï¼š\n"
                for note in matched_notes:
                    output += f"   â€¢ {note.title}\n"
                    if note.desc:
                        output += f"     ç®€ä»‹ï¼š{note.desc[:100]}...\n"
                    output += f"     é“¾æ¥ï¼šhttps://www.xiaohongshu.com/explore/{note.note_id}\n"
                    output += "\n"
            else:
                output += "   ï¼ˆæš‚æ— ç›¸å…³ç¬”è®°æ¨èï¼‰\n"
            
            output += "\n" + "-" * 50 + "\n\n"
        
        return output
    
    def process(self, goal: str, user_ids: List[str], debug: bool = False) -> str:
        """
        å¤„ç†ç”¨æˆ·è¯·æ±‚çš„ä¸»å‡½æ•°
        :param goal: ç”¨æˆ·ç›®æ ‡
        :param user_ids: ç”¨æˆ·å…³æ³¨çš„user_idåˆ—è¡¨
        :param debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        :return: æ ¼å¼åŒ–çš„å­¦ä¹ è®¡åˆ’
        """
        logger.info(f"å¼€å§‹å¤„ç†ç›®æ ‡: {goal}")
        logger.info(f"å…³æ³¨ç”¨æˆ·æ•°: {len(user_ids)}")
        
        # è·å–ç”¨æˆ·ç¬”è®°
        user_notes = self.get_user_notes(user_ids)
        logger.info(f"è·å–åˆ° {len(user_notes)} æ¡ç¬”è®°")
        
        if debug:
            print("\n" + "=" * 60)
            print("ğŸ“ è°ƒè¯•ä¿¡æ¯ï¼šè·å–åˆ°çš„ç¬”è®°")
            print("=" * 60)
            print(f"æ€»å…±è·å–åˆ° {len(user_notes)} æ¡ç¬”è®°\n")
            
            # æ˜¾ç¤ºå‰10æ¡ç¬”è®°
            for i, note in enumerate(user_notes[:10], 1):
                print(f"{i}. {note.title}")
                print(f"   æ ‡ç­¾: {', '.join(note.tags) if note.tags else 'æ— '}")
                print(f"   æè¿°: {note.desc[:80]}...")
                print()
            
            if len(user_notes) > 10:
                print(f"... è¿˜æœ‰ {len(user_notes) - 10} æ¡ç¬”è®°æœªæ˜¾ç¤º\n")
        
        # æ‹†è§£ç›®æ ‡
        if debug:
            print("=" * 60)
            print("ğŸ” æ­£åœ¨æ‹†è§£å­¦ä¹ ç›®æ ‡...")
            print("=" * 60)
        
        result = self.decompose_goal(goal, user_notes)
        
        if debug:
            print("\n" + "=" * 60)
            print("ğŸ“‹ ç”Ÿæˆçš„æ­¥éª¤åŠåŒ¹é…æƒ…å†µ")
            print("=" * 60)
            for i, step in enumerate(result['steps'], 1):
                print(f"\næ­¥éª¤ {i}: {step}")
                matched = result['matched_notes'].get(step, [])
                print(f"  åŒ¹é…åˆ° {len(matched)} æ¡ç¬”è®°")
                if matched:
                    for j, note in enumerate(matched, 1):
                        print(f"    {j}. {note.title}")
                        print(f"       æ ‡ç­¾: {', '.join(note.tags) if note.tags else 'æ— '}")
                else:
                    print("    ï¼ˆæ— åŒ¹é…ç¬”è®°ï¼‰")
            print("\n" + "=" * 60 + "\n")
        
        # æ ¼å¼åŒ–è¾“å‡º
        return self.format_output(result)